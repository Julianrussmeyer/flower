# FlowerTune LLM Evaluation

This repository provides various evaluation metrics to measure the quality of your finetuned LLMs.
As the final steps to participate in [LLM Leaderboard](https://flower.ai/benchmarks/llm-leaderboard#how-to-participate),
the evaluation scores generated here will be displayed as the definitive values on the LLM Leaderboard.

## How to run
Please check out the individual directory corresponding to your selected challenge (general NLP, finance, medical, and code) to learn how to execute the evaluations.

> [!NOTE]  
> If you wish to participate in the LLM Leaderboard, you must not modify the evaluation code and should use the exact command provided in the respective directory to run the evaluation.


## Expected results
The default template generated by `flwr new` for each challenge will produce results as follows, which serve as the lower bound on the LLM Leaderboard.

### General NLP

|          | MT-1 | MT-2 | MT-Avg |  
|:--------:|:----:|:----:|:------:|
| MT Score | 5.54 | 5.52 |  5.53  |

### Finance

|         |  FPB  | FIQA  | TFNS  |  Avg  |  
|:-------:|:-----:|:-----:|:-----:|:-----:|
| Acc (%) | 44.55 | 63.64 | 28.77 | 45.65 |

### Medical

|         | PubMedQA | MedMCQA | MedQA |  Avg  |  
|:-------:|:--------:|:-------:|:-----:|:-----:|
| Acc (%) |  59.00   |  23.69  | 27.10 | 36.60 |

### Code

|            | MBPP  | HumanEval | Multiple (JS) | Multiple (C++) |  Avg  |  
|:----------:|:-----:|:---------:|:-------------:|:--------------:|:-----:|
| Pass@1 (%) | 31.40 |   25.00   |     31.68     |     24.84      | 28.23 |
